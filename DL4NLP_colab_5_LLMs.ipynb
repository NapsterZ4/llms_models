{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using LLMs with LangChain and Groq\n",
        "\n",
        "**LangChain**  is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
        "\n",
        "In many ways, LangChain is essentially an abstraction layer on top of language models to improve the connection between them or different software applications (hence the \"chain\" part in the name).\n",
        "\n",
        "Check:\n",
        "\n",
        "* LangChain tutorials: https://python.langchain.com/v0.2/docs/tutorials/\n",
        "\n",
        "* How-to guides: https://python.langchain.com/v0.2/docs/how_to/\n",
        "\n",
        "\n",
        "\n",
        "**Groq** is the AI infrastructure company that delivers fast AI inference.  The LPUâ„¢ Inference Engine by Groq is a hardware and software platform that delivers exceptional compute speed, quality, and energy efficiency. Groq, headquartered in Silicon Valley, provides cloud and on-prem solutions at scale for AI applications. The LPU and related systems are designed, fabricated, and assembled in North America.  \n",
        "\n",
        "*   Check LangChain integration with Groq https://python.langchain.com/v0.1/docs/integrations/chat/groq/\n",
        "*   Try Groq playground here: https://console.groq.com/playground.\n",
        "\n",
        "*   **Get your API key** after login here: https://console.groq.com/keys.  \n",
        "\n",
        "**NOTE**: the following code load a secret with the key GROQKEY. Use the *secrets* tab in Google Colab to store your own Groq API key.\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "0oGG1x79SQ8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq\n",
        "from google.colab import userdata\n",
        "myKey= userdata.get('GROQKEY')"
      ],
      "metadata": {
        "id": "jtJ0dZjhxdXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Mixtral LLM\n",
        "\n",
        "Mistral AI is a French company specializing in artificial intelligence (AI) products founded in April 2023. Mixtral 8x7B was released via a BitTorrent link posted on Twitter on December 9, 2023 and later Hugging Face and a blog post were released two days later. It uses a sparse mixture of experts architecture.\n",
        "\n",
        "Let us connect with a popular LLM in 2024, *mixtral-8x7b-32768*, and ask it to explain the NLP tasks that a LLM is useful for.\n",
        "\n",
        "The temperature parameter controls randomness: lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\n"
      ],
      "metadata": {
        "id": "Id6m0Ay3SdyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat = ChatGroq(temperature=0, groq_api_key=myKey, model_name=\"mixtral-8x7b-32768\") #temperature and model selection\n",
        "\n",
        "system = \"You are a helpful assistant.\" #system role or preprompt\n",
        "human = \"{text}\" #template string human with a placeholder {text}. This placeholder will be filled with actual user input when the prompt is invoked.\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)]) #template string human with a placeholder {text}. This placeholder will be filled with actual user input when the prompt is invoked.\n",
        "\n",
        "chain = prompt | chat #This line creates a chain  (a sequence of operations) of LangChain by combining the prompt template with a chat object, which represents the language model or chatbot. The | operator is used to pipe the prompt into the chat model, indicating that the output of the prompt template should be the input to the chat model.\n",
        "\n",
        "response = chain.invoke({\"text\": \"Explain the NLP tasks that a LLM is useful for.\"}) #This line runs the chain with a specific input. It passes a dictionary containing the key \"text\" with the value \"Explain the importance of low latency LLMs.\" to the chain. This value will replace the {text} placeholder in the human message template. The chain processes this input, generating a response from the language model.\n",
        "\n",
        "print(response.content) #type AIMessage of LangChain which include \"content\" among others"
      ],
      "metadata": {
        "id": "vfoUbQexBydD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The system role or preprompt\n",
        "\n",
        "The system role or preprompt in LLMs is essential for shaping the interaction between the model and users. It provides the necessary context, behavioral guidelines, task-specific instructions, safety constraints, user personalization, and domain-specific knowledge integration. By defining a clear system role, LLMs can deliver more accurate, relevant, and responsible responses, enhancing the overall user experience.\n",
        "\n",
        "Some examples are:\n",
        "\n",
        "* *Customer Support Bot*: The system role is defined to handle common customer  queries, provide troubleshooting steps, and escalate issues to human agents when necessary.\n",
        "* *Educational Tutor*: The system role focuses on providing explanations of educational content.\n",
        "* *Creative Writing Assistant*: The system role encourages creative and imaginative responses.\n",
        "\n",
        "\n",
        "So...instead of *You are a helpful assistant*, now you are a pirate. Also,  let us add more randomness/temperature (between 0 and 2 according to the playground)."
      ],
      "metadata": {
        "id": "iQ-_vv3-FIQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatGroq(temperature=1, groq_api_key=myKey, model_name=\"mixtral-8x7b-32768\") #in playground, temperature is\n",
        "\n",
        "system = \"You are a pirate.\" #system role or preprompt\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | chat\n",
        "response = chain.invoke({\"text\": \"Explain the NLP tasks that a LLM is useful for.\"})\n",
        "\n",
        "print(response.content) #type AIMessage which include \"content\" among others"
      ],
      "metadata": {
        "id": "kLSGf1rIDle2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text classification with LLMs\n",
        "\n",
        "Let us do some old fashion text classification with very simple changes in the previous code.\n",
        "\n",
        "We will use the next preprompt: *You are a machine learning model trained for Text Classification. You take a list of business reviews as input and return the starts of the review (1 to 5). Expresses the output in json format, with two fields: review, and prediction.*\n",
        "\n",
        "Limiting the output format in a json file, it is trivial to connect an application that provides the input at the prompt and takes the response for use.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4CdO-jt4LHDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatGroq(temperature=0, groq_api_key=myKey, model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "system = \"You are a machine learning model trained for Text Classification. You take a list of business reviews as input and return the starts of the review (1 to 5). Expresses the output in json format, with two fields: review, and prediction.\" #system role or preprompt\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "new_reviews = [\n",
        "    \"The food was amazing and the service was excellent.\",\n",
        "    \"I did not enjoy the food at all.\",\n",
        "    \"The ambiance was nice but the food was just okay.\",\n",
        "    \"Terrible experience! Will not come back.\",\n",
        "    \"Best restaurant ever! Highly recommend.\",\n",
        "    \"Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.\"\n",
        "]\n",
        "\n",
        "chain = prompt | chat\n",
        "response = chain.invoke({\"text\": new_reviews})\n",
        "\n",
        "print(response.content) #type AIMessage which include \"content\" among others\n"
      ],
      "metadata": {
        "id": "OqmzLpqHLUyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Changing the LLM\n",
        "Last review is tricky: *Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.*\n",
        "\n",
        "Let us try another LLM supported by the groqcloud: https://console.groq.com/docs/models  \n",
        "\n"
      ],
      "metadata": {
        "id": "V7tlG2OnPpss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatGroq(temperature=0, groq_api_key=myKey, model_name=\"llama3-70b-8192\")\n",
        "\n",
        "system = \"You are a machine learning model trained for Text Classification. You take a list of business reviews as input and return the starts of the review (1 to 5). Expresses the output in json format, with two fields: review, and prediction.\" #system or preprint\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "new_reviews = [\n",
        "    \"The food was amazing and the service was excellent.\",\n",
        "    \"I did not enjoy the food at all.\",\n",
        "    \"The ambiance was nice but the food was just okay.\",\n",
        "    \"Terrible experience! Will not come back.\",\n",
        "    \"Best restaurant ever! Highly recommend.\",\n",
        "    \"Bad and expensive food, noisy and unpleasant atmosphere, slow and rude waiters. That's what you'll find in other restaurants, but here it's completely the opposite.\"\n",
        "]\n",
        "\n",
        "\n",
        "chain = prompt | chat\n",
        "response = chain.invoke({\"text\": new_reviews})\n",
        "\n",
        "print(response.content) #type AIMessage which include \"content\" among others"
      ],
      "metadata": {
        "id": "2tfGQ9A1P0oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#More tasks with LLMs\n",
        "\n",
        "This same general scheme can be used to design systems that require any of the NLP tasks that an LLM can address, such as machine translation or text summarization.\n",
        "\n",
        "\n",
        "You can even use  LLMs to generate good prompts: *Generate detailed preprompt  for the system role of an LLM designed to summarize scientific papers in the field of artificial intelligence*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GIJ7vLvoPYjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "chat = ChatGroq(temperature=0, groq_api_key=myKey, model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "\n",
        "system = \"You are a helpful assistant.\" #system role or preprompt\n",
        "human = \"{text}\"\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
        "\n",
        "chain = prompt | chat\n",
        "response = chain.invoke({\"text\": \"Generate detailed preprompt for the system role of an LLM designed to summarize scientific papers in the field of artificial intelligence.\"})\n",
        "\n",
        "print(response.content) #type AIMessage which include \"content\" among others"
      ],
      "metadata": {
        "id": "QVcR27OZU5Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "We have reviewed the basic use of LangChain to create applications based on or connected to LLMs. Specifically, we have used the LangChain connector with GRoqCloud, which offers an API for efficient access to various language models in the cloud.\n",
        "\n",
        "The use of the system role or preprompt as well as limiting the output produced by the LLM are key to connecting these models with other software.\n",
        "\n",
        "More material in LangChain tutorials (https://python.langchain.com/v0.2/docs/tutorials/) and How-to guides (https://python.langchain.com/v0.2/docs/how_to/).\n",
        "\n"
      ],
      "metadata": {
        "id": "HgEAcvPBqIGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Things to try\n",
        "\n",
        "*   Generate a good preprompt and prompt for a task of your choice and then use LLMs to solve it.\n",
        "\n",
        "*   Explore (and reproduce) the [Classify Text into Labels](https://python.langchain.com/v0.2/docs/tutorials/classification/) tutorial of LangChain which gives developers more control over the LLMs output.\n"
      ],
      "metadata": {
        "id": "V2TmRH4Rt3dp"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}