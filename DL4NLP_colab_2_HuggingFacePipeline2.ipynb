{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import transformers"
      ],
      "metadata": {
        "id": "vJVVXlkKOrYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named entity recognition\n",
        "Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations.\n",
        "\n",
        "We pass the option ***grouped_entities=True*** in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity."
      ],
      "metadata": {
        "id": "4njzUek3PB89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto.\")"
      ],
      "metadata": {
        "id": "q3WmMWhvOvBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization\n",
        "Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text."
      ],
      "metadata": {
        "id": "gxjjXjAOQUWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "\"\"\"\n",
        "Symbolic NLP (1950s – early 1990s)\n",
        "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n",
        "\n",
        "1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[1] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted in America (though some research continued elsewhere, such as Japan and Europe[2]) until the late 1980s when the first statistical machine translation systems were developed.\n",
        "1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\". Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in a computer memory at the time.[3]\n",
        "1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first chatterbots were written (e.g., PARRY).\n",
        "1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[4]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[5]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[6]\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "Bk9aNuMvQdX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question answering\n",
        "The question-answering pipeline answers questions using information from a given contex."
      ],
      "metadata": {
        "id": "i2c3eVkkSbp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"When does Geoffrey Everest Hinton worked at Google?\",\n",
        "    context=\"Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. From 2013 to 2023, he divided his time working for Google (Google Brain) and the University of Toronto.\",\n",
        ")"
      ],
      "metadata": {
        "id": "CKKrFZmERbUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation\n",
        "\n",
        "A default model if can be used when providing  a language pair in the task name (such as \"translation_en_to_fr\"), but the easiest way is to get the model you want to use on the [Model Hub](https://huggingface.co/models) after selecting a language. Let us try English to Spanish.\n",
        "\n",
        "**Note**: Check the models examples and API to find parameters. Import require libraries.\n",
        "\n",
        "**Note 2**: sentencepiece is usually needed, if this error show up *\"ValueError: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.\"*, try *\"!pip install sentencepiece\"* and restart the kernel\""
      ],
      "metadata": {
        "id": "hl3R6QUHS3SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "import sentencepiece"
      ],
      "metadata": {
        "id": "XFn3naAF9Egw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
        "translator(\"What can I learn in Deep Learning for Natural Language Processing?\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IUOm3Gs4TMxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mask filling\n",
        "\n",
        "This task can fill in the blanks in a given text.\n",
        "\n",
        "The **top_k** argument controls how many possibilities you want to be displayed.\n",
        "\n",
        "Note that here the model fills in the special <mask> word, which is often referred to as a mask token. Other mask-filling models might have different mask tokens, so it’s always good to verify the proper mask word when exploring other models (check the API widget).\n",
        "\n"
      ],
      "metadata": {
        "id": "sdk9U03RX0gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian <mask>, most noted for his work on artificial neural networks.\", top_k=5)"
      ],
      "metadata": {
        "id": "SqtpFr7QX7AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bias\n",
        "Beware of bias in pretrained model languages!\n",
        "\n"
      ],
      "metadata": {
        "id": "vLPy4AQzZP1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"This man works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n"
      ],
      "metadata": {
        "id": "pErVOLr-ZdRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = unmasker(\"This woman works as a [MASK].\")\n",
        "print([r[\"token_str\"] for r in result])"
      ],
      "metadata": {
        "id": "mlBenmiV9lwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}